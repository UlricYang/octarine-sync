# 数据结构设计

## 用链表加强哈希表 - LinkedHashMap

标准哈希表的键是无序存储在底层的 table 数组中的

我们现在希望在不改变哈希表增删查改复杂度的前提下，能够按照插入顺序来访问所有键，且不受扩缩容影响
那么一个最直接的思路就是：我想办法把这些键值对都用类似链表节点的结构串联起来，持有一个头尾结点 head, tail 的引用，每次把新的键插入 table 数组时，同时把这个键插入链表的尾部。这样一来，只要我从头结点 head 开始遍历链表，就能按照插入顺序访问所有键了

- 我们还是可以在O(1) 的时间复杂度内通过键查找到对应的双链表节点，进而找到键对应的值
- 我们可以在O(1) 的时间复杂度内插入新的键值对。因为哈希表本身的插入操作时间复杂度是O(1)，且双链表头尾的插入操作时间复杂度也是O(1)
- 我们可以在O(1) 的时间复杂度内删除指定的键值对。因为哈希表本身的删除操作时间复杂度是O(1)，删除给定双链表节点的操作时间复杂度也是O(1)
- 由于链表节点的顺序是插入顺序，那么只要从头结点开始遍历这个链表，就能按照插入顺序访问所有键

## LRU

LRU 的全称是 Least Recently Used，也就是说我们认为最近使用过的数据应该是是「有用的」，很久都没用过的数据应该是无用的，内存满了就优先删那些很久没用过的数据

LRU 算法用到的关键数据结构是哈希链表 LinkedHashMap

![](https://raw.githubusercontent.com/UlricYang/FigureBed/main/img/20251118162028628.jpg)

先不慌去实现 LRU 算法的 get 和 put 方法。由于我们要同时维护一个双链表 cache 和一个哈希表 map，很容易漏掉一些操作，比如说删除某个 key 时，在 cache 中删除了对应的 Node，但是却忘记在 map 中删除 key。解决这种问题的有效方法是：在这两种数据结构之上提供一层抽象 API。就是尽量让 LRU 的主方法 get 和 put 避免直接操作 map 和 cache 的细节

## 用数组加强哈希表 - LinkedHashSet

如何用数组加强哈希表，轻松实现 randomKey() API：用一个数组 arr 维护哈希表中所有的键，然后通过随机索引返回一个键。这样就能保证均匀随机，且时间复杂度是 O(1)

O(1) 时间删除数组中的任意元素：把待删除的元素，先交换到数组尾部，然后再删除，数组尾部删除元素的时间复杂度是 O(1)。当然，这样的代价就是数组中的元素顺序会被打乱，但是对于我们当前的场景来说，数组中的元素顺序并不重要，所以打乱了也无所谓

如何知道一个元素在数组中对应的索引：正常来说需要遍历数组寻找目标元素，这样时间复杂度是O(N)；但是现在不是有哈希表么，键值映射是干啥的？不就是帮你优化这种需要傻乎乎遍历的场景的么？也就是说，你可以在哈希表中建立数组元素到数组索引的映射关系，这样就能在 O(1) 的时间复杂度内找到数组元素对应的索引了

## LFU

LFU 算法的淘汰策略是 Least Frequently Used，也就是每次淘汰那些使用次数最少的数据

LFU 算法相当于是把数据按照访问频次进行排序，这个需求恐怕没有那么简单，而且还有一种情况，如果多个数据拥有相同的访问频次，我们就得删除最早插入的那个数据。也就是说 LFU 算法是淘汰访问频次最低的数据，如果访问频次最低的数据有多条，需要淘汰最旧的数据

LinkedHashSet 顾名思义，是链表和哈希集合的结合体。链表不能快速访问链表节点，但是插入元素具有时序；哈希集合中的元素无序，但是可以对元素进行快速的访问和删除。那么，它俩结合起来就兼具了哈希集合和链表的特性，既可以在 O(1) 时间内访问或删除其中的元素，又可以保持插入的时序

LFU 的逻辑不难理解，但是写代码实现并不容易，因为你看我们要维护 KV 表，KF 表，FK 表三个映射，特别容易出错。对于这种情况，我教你三个技巧：

1. 不要企图上来就实现算法的所有细节，而应自顶向下，逐步求精，先写清楚主函数的逻辑框架，然后再一步步实现细节
1. 搞清楚映射关系，如果我们更新了某个 key 对应的 freq，那么就要同步修改 KF 表和 FK 表，这样才不会出问题
1. 画图，画图，画图，重要的话说三遍，把逻辑比较复杂的部分用流程图画出来，然后根据图来写代码，可以极大减少出错的概率
